{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88b0f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T17:20:20.286675Z",
     "iopub.status.busy": "2024-09-14T17:20:20.286122Z",
     "iopub.status.idle": "2024-09-14T17:20:28.109309Z",
     "shell.execute_reply": "2024-09-14T17:20:28.108349Z"
    },
    "papermill": {
     "duration": 7.832331,
     "end_time": "2024-09-14T17:20:28.111924",
     "exception": false,
     "start_time": "2024-09-14T17:20:20.279593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import wandb\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832ee2a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T17:20:28.122291Z",
     "iopub.status.busy": "2024-09-14T17:20:28.121387Z",
     "iopub.status.idle": "2024-09-14T17:20:28.197699Z",
     "shell.execute_reply": "2024-09-14T17:20:28.196662Z"
    },
    "papermill": {
     "duration": 0.083562,
     "end_time": "2024-09-14T17:20:28.199822",
     "exception": false,
     "start_time": "2024-09-14T17:20:28.116260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model definitions (SEBlock, MultiHeadAttention, RadioNet) should be here, \n",
    "# but they're omitted for brevity. Make sure they're included in your actual script.\n",
    "\n",
    "def load_test_data(dataset_path, selected_classes_id, selected_modulation_classes, N_SNR=4):\n",
    "    with h5py.File(dataset_path, \"r\") as dataset_file:\n",
    "        X_data = None\n",
    "        y_data = None\n",
    "\n",
    "        for id in selected_classes_id:\n",
    "            X_slice = dataset_file['X'][(106496*(id+1) - 4096*N_SNR):106496*(id+1)]\n",
    "            y_slice = dataset_file['Y'][(106496*(id+1) - 4096*N_SNR):106496*(id+1)]\n",
    "            \n",
    "            if X_data is not None:\n",
    "                X_data = np.concatenate([X_data, X_slice], axis=0)\n",
    "                y_data = np.concatenate([y_data, y_slice], axis=0)\n",
    "            else:\n",
    "                X_data = X_slice\n",
    "                y_data = y_slice\n",
    "\n",
    "    X_data = X_data.reshape(len(X_data), 32, 32, 2)\n",
    "    y_data_df = pd.DataFrame(y_data)\n",
    "\n",
    "    print(f\"Original y_data shape: {y_data_df.shape}\")\n",
    "    print(f\"Columns with non-zero sum: {(y_data_df.sum() != 0).sum()}\")\n",
    "\n",
    "    non_zero_columns = y_data_df.columns[y_data_df.sum() != 0]\n",
    "    y_data_df = y_data_df[non_zero_columns]\n",
    "\n",
    "    print(f\"Filtered y_data shape: {y_data_df.shape}\")\n",
    "\n",
    "    if y_data_df.empty:\n",
    "        raise ValueError(\"All columns in y_data have zero sum. Check your data and selected classes.\")\n",
    "\n",
    "    y_indices = np.argmax(y_data_df.values, axis=1)\n",
    "    \n",
    "    class_mapping = {i: cls for i, cls in enumerate(selected_modulation_classes)}\n",
    "    mapped_classes = [class_mapping[i] for i in y_indices]\n",
    "\n",
    "    return X_data, y_indices, selected_modulation_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80eb63fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T17:20:28.209672Z",
     "iopub.status.busy": "2024-09-14T17:20:28.208861Z",
     "iopub.status.idle": "2024-09-14T17:20:29.648193Z",
     "shell.execute_reply": "2024-09-14T17:20:29.647062Z"
    },
    "papermill": {
     "duration": 1.446702,
     "end_time": "2024-09-14T17:20:29.650552",
     "exception": false,
     "start_time": "2024-09-14T17:20:28.203850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "my_secret = user_secrets.get_secret(\"wandb_api_key\") \n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece83b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T17:20:29.661370Z",
     "iopub.status.busy": "2024-09-14T17:20:29.660980Z",
     "iopub.status.idle": "2024-09-14T17:20:29.688327Z",
     "shell.execute_reply": "2024-09-14T17:20:29.687305Z"
    },
    "papermill": {
     "duration": 0.035534,
     "end_time": "2024-09-14T17:20:29.690590",
     "exception": false,
     "start_time": "2024-09-14T17:20:29.655056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline, X_test, y_test, class_names):\n",
    "    wandb.init(project=\"radioml-evaluation\", name=\"pipeline-evaluation\")\n",
    "\n",
    "    # Ensure X_test is a tensor and on the correct device\n",
    "    if not isinstance(X_test, torch.Tensor):\n",
    "        X_test = torch.from_numpy(X_test).float()\n",
    "    X_test = X_test.to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "    # Move predictions back to CPU for sklearn metrics\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "    if isinstance(y_pred_proba, torch.Tensor):\n",
    "        y_pred_proba = y_pred_proba.cpu().numpy()\n",
    "\n",
    "    # Print shapes and unique values\n",
    "    print(f\"y_test shape: {y_test.shape}, unique values: {np.unique(y_test)}\")\n",
    "    print(f\"y_pred shape: {y_pred.shape}, unique values: {np.unique(y_pred)}\")\n",
    "    print(f\"Number of class_names: {len(class_names)}\")\n",
    "\n",
    "    # Ensure y_test and y_pred are 1D arrays\n",
    "    y_test = y_test.ravel()\n",
    "    y_pred = y_pred.ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Use np.unique to get the actual labels present in the data\n",
    "    labels = np.unique(np.concatenate((y_test, y_pred)))\n",
    "    class_report = classification_report(y_test, y_pred, labels=labels, target_names=[class_names[i] for i in labels], output_dict=True)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\n",
    "        \"accuracy\": accuracy,\n",
    "        \"classification_report\": wandb.Table(dataframe=pd.DataFrame(class_report).transpose())\n",
    "    })\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    n_classes = len(class_names)\n",
    "    y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    wandb.log({\"roc_curve\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "        plt.plot(recall[i], precision[i], lw=2, \n",
    "                 label=f'{class_names[i]} (AP = {average_precision[i]:.2f})')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    wandb.log({\"precision_recall_curve\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "\n",
    "    # Feature Importances\n",
    "    if hasattr(pipeline.named_steps['xgb_classifier'], 'feature_importances_'):\n",
    "        feature_imp = pipeline.named_steps['xgb_classifier'].feature_importances_\n",
    "        feature_imp_df = pd.DataFrame({'feature': [f'feature_{i}' for i in range(len(feature_imp))],\n",
    "                                       'importance': feature_imp})\n",
    "        feature_imp_df = feature_imp_df.sort_values('importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_imp_df.head(20))\n",
    "        plt.title('Top 20 Feature Importances')\n",
    "        wandb.log({\"feature_importances\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "\n",
    "        wandb.log({\"feature_importance_table\": wandb.Table(dataframe=feature_imp_df)})\n",
    "    else:\n",
    "        print(\"Feature importances not available for this model.\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e467f43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T17:20:29.701117Z",
     "iopub.status.busy": "2024-09-14T17:20:29.700698Z",
     "iopub.status.idle": "2024-09-14T17:20:29.727431Z",
     "shell.execute_reply": "2024-09-14T17:20:29.726308Z"
    },
    "papermill": {
     "duration": 0.034788,
     "end_time": "2024-09-14T17:20:29.729801",
     "exception": false,
     "start_time": "2024-09-14T17:20:29.695013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    \"\"\" Squeeze-and-Excitation Block \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = self.se(x)\n",
    "        return x * scale\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention Module \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class RadioNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(RadioNet, self).__init__()\n",
    "\n",
    "        # Separate Convolutional Pathways for I and Q\n",
    "        self.q_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(64),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(256),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.i_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(64),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(256),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.feature_size = self._get_conv_output((1, 32, 32))\n",
    "\n",
    "        # Bidirectional LSTM with Layer Normalization\n",
    "        self.lstm = nn.LSTM(self.feature_size * 2, 512, num_layers=2, \n",
    "                            batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.layer_norm = nn.LayerNorm(1024)  # Layer normalization after LSTM\n",
    "\n",
    "        # Multi-Head Attention with multiple heads\n",
    "        self.multi_head_attn = MultiHeadAttention(1024, num_heads=8)\n",
    "\n",
    "        # Enhanced Fully Connected Layers with Dense Connections\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        input = torch.rand(1, *shape)\n",
    "        output = self.q_conv(input)\n",
    "        return int(torch.numel(output) / output.shape[0])\n",
    "\n",
    "    def forward(self, i_input, q_input):\n",
    "        q = self.q_conv(q_input)\n",
    "        q = q.view(q.size(0), -1)\n",
    "\n",
    "        i = self.i_conv(i_input)\n",
    "        i = i.view(i.size(0), -1)\n",
    "\n",
    "        combined = torch.cat((q, i), dim=1)\n",
    "        combined = combined.unsqueeze(1)  # Add sequence dimension\n",
    "\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        # Apply Multi-Head Attention\n",
    "        attn_output = self.multi_head_attn(lstm_out)\n",
    "        context = torch.sum(attn_output, dim=1)  # Sum up the attended output\n",
    "\n",
    "        x = self.fc(context)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cbd72ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T17:20:29.740310Z",
     "iopub.status.busy": "2024-09-14T17:20:29.739898Z",
     "iopub.status.idle": "2024-09-14T17:20:29.750247Z",
     "shell.execute_reply": "2024-09-14T17:20:29.749268Z"
    },
    "papermill": {
     "duration": 0.018379,
     "end_time": "2024-09-14T17:20:29.752642",
     "exception": false,
     "start_time": "2024-09-14T17:20:29.734263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FrozenFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            features_list = []\n",
    "            for batch in DataLoader(X, batch_size=32):\n",
    "                batch = batch.to(device)\n",
    "                i_input = batch[:, :, :, 0].unsqueeze(1)\n",
    "                q_input = batch[:, :, :, 1].unsqueeze(1)\n",
    "\n",
    "                features = self.model.fc(self.model.multi_head_attn(self.model.layer_norm(self.model.lstm(torch.cat((\n",
    "                    self.model.q_conv(q_input).view(q_input.size(0), -1),\n",
    "                    self.model.i_conv(i_input).view(i_input.size(0), -1)\n",
    "                ), dim=1).unsqueeze(1))[0])).sum(dim=1))\n",
    "\n",
    "                features_list.append(features.cpu().numpy())\n",
    "\n",
    "            return np.vstack(features_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd982e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T17:20:29.763142Z",
     "iopub.status.busy": "2024-09-14T17:20:29.762727Z",
     "iopub.status.idle": "2024-09-14T17:22:27.254334Z",
     "shell.execute_reply": "2024-09-14T17:22:27.253166Z"
    },
    "papermill": {
     "duration": 117.499923,
     "end_time": "2024-09-14T17:22:27.257056",
     "exception": false,
     "start_time": "2024-09-14T17:20:29.757133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y_data shape: (131072, 24)\n",
      "Columns with non-zero sum: 8\n",
      "Filtered y_data shape: (131072, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pipeline from /kaggle/input/xgbnet/pytorch/default/1/trained_pipeline.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdevcode03\u001b[0m (\u001b[33mdevcode03-gujarat-technological-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240914_172047-n6cfqgco\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpipeline-evaluation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/radioml-evaluation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/radioml-evaluation/runs/n6cfqgco\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape: (131072,), unique values: [0 1 2 3 4 5 6 7]\n",
      "y_pred shape: (131072,), unique values: [0 1 2 3 4 5 6 7]\n",
      "Number of class_names: 8\n",
      "Accuracy: 0.9959\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        4ASK       1.00      1.00      1.00     16384\n",
      "        BPSK       1.00      1.00      1.00     16384\n",
      "        QPSK       1.00      1.00      1.00     16384\n",
      "       16PSK       1.00      1.00      1.00     16384\n",
      "       16QAM       0.98      0.99      0.98     16384\n",
      "          FM       0.99      0.98      0.98     16384\n",
      "   AM-DSB-WC       1.00      1.00      1.00     16384\n",
      "      32APSK       1.00      1.00      1.00     16384\n",
      "\n",
      "    accuracy                           1.00    131072\n",
      "   macro avg       1.00      1.00      1.00    131072\n",
      "weighted avg       1.00      1.00      1.00    131072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: accuracy 0.99595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpipeline-evaluation\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/radioml-evaluation/runs/n6cfqgco\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/radioml-evaluation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 6 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240914_172047-n6cfqgco/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "dataset_path = \"/kaggle/input/radioml2018/GOLD_XYZ_OSC.0001_1024.hdf5\"\n",
    "pipeline_path = \"/kaggle/input/xgbnet/pytorch/default/1/trained_pipeline.joblib\"\n",
    "\n",
    "base_modulation_classes = [\n",
    "    'OOK', '4ASK', '8ASK', 'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK',\n",
    "    '16APSK', '32APSK', '64APSK', '128APSK', '16QAM', '32QAM', '64QAM',\n",
    "    '128QAM', '256QAM', 'AM-SSB-WC', 'AM-SSB-SC', 'AM-DSB-WC', 'AM-DSB-SC',\n",
    "    'FM', 'GMSK', 'OQPSK'\n",
    "]\n",
    "selected_modulation_classes = [\n",
    "    '4ASK', 'BPSK', 'QPSK', '16PSK', '16QAM', 'FM', 'AM-DSB-WC', '32APSK'\n",
    "]\n",
    "selected_classes_id = [base_modulation_classes.index(cls) for cls in selected_modulation_classes]\n",
    "\n",
    "# Load test data\n",
    "X_test, y_test, class_names = load_test_data(dataset_path, selected_classes_id, selected_modulation_classes)\n",
    "\n",
    "# Load the trained pipeline\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "print(f\"Loaded pipeline from {pipeline_path}\")\n",
    "\n",
    "# Move the feature extractor to the correct device\n",
    "pipeline.named_steps['feature_extraction'].model.to(device)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "evaluate_pipeline(pipeline, X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79087f2e",
   "metadata": {
    "papermill": {
     "duration": 0.005924,
     "end_time": "2024-09-14T17:22:27.269437",
     "exception": false,
     "start_time": "2024-09-14T17:22:27.263513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1493018,
     "sourceId": 2468162,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 119373,
     "modelInstanceId": 95172,
     "sourceId": 113431,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 132.79232,
   "end_time": "2024-09-14T17:22:29.895250",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-14T17:20:17.102930",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
