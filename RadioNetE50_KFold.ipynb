{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56169b86",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-11T15:37:20.291638Z",
     "iopub.status.busy": "2024-09-11T15:37:20.291236Z",
     "iopub.status.idle": "2024-09-11T15:37:27.353904Z",
     "shell.execute_reply": "2024-09-11T15:37:27.352956Z"
    },
    "papermill": {
     "duration": 7.072419,
     "end_time": "2024-09-11T15:37:27.356458",
     "exception": false,
     "start_time": "2024-09-11T15:37:20.284039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import h5py\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca8a196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T15:37:27.369337Z",
     "iopub.status.busy": "2024-09-11T15:37:27.368776Z",
     "iopub.status.idle": "2024-09-11T15:37:27.388170Z",
     "shell.execute_reply": "2024-09-11T15:37:27.387278Z"
    },
    "papermill": {
     "duration": 0.027752,
     "end_time": "2024-09-11T15:37:27.390179",
     "exception": false,
     "start_time": "2024-09-11T15:37:27.362427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "dataset_file = h5py.File(\"/kaggle/input/radioml2018/GOLD_XYZ_OSC.0001_1024.hdf5\", \"r\")\n",
    "\n",
    "# Base modulation classes\n",
    "base_modulation_classes = [\n",
    "    'OOK', '4ASK', '8ASK', 'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK',\n",
    "    '16APSK', '32APSK', '64APSK', '128APSK', '16QAM', '32QAM', '64QAM',\n",
    "    '128QAM', '256QAM', 'AM-SSB-WC', 'AM-SSB-SC', 'AM-DSB-WC', 'AM-DSB-SC',\n",
    "    'FM', 'GMSK', 'OQPSK'\n",
    "]\n",
    "\n",
    "# Selected modulation classes\n",
    "selected_modulation_classes = [\n",
    "    '4ASK', 'BPSK', 'QPSK', '16PSK', '16QAM', 'FM', 'AM-DSB-WC', '32APSK'\n",
    "]\n",
    "\n",
    "# Get the indices of selected modulation classes\n",
    "selected_classes_id = [base_modulation_classes.index(cls) for cls in selected_modulation_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6429a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T15:37:27.401572Z",
     "iopub.status.busy": "2024-09-11T15:37:27.401221Z",
     "iopub.status.idle": "2024-09-11T15:37:27.428216Z",
     "shell.execute_reply": "2024-09-11T15:37:27.427154Z"
    },
    "papermill": {
     "duration": 0.035469,
     "end_time": "2024-09-11T15:37:27.430566",
     "exception": false,
     "start_time": "2024-09-11T15:37:27.395097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\" Squeeze-and-Excitation Block \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = self.se(x)\n",
    "        return x * scale\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention Module \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class RadioNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(RadioNet, self).__init__()\n",
    "\n",
    "        # Separate Convolutional Pathways for I and Q\n",
    "        self.q_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(64),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(256),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.i_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(64),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(256),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.feature_size = self._get_conv_output((1, 32, 32))\n",
    "\n",
    "        # Bidirectional LSTM with Layer Normalization\n",
    "        self.lstm = nn.LSTM(self.feature_size * 2, 512, num_layers=2, \n",
    "                            batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.layer_norm = nn.LayerNorm(1024)  # Layer normalization after LSTM\n",
    "\n",
    "        # Multi-Head Attention with multiple heads\n",
    "        self.multi_head_attn = MultiHeadAttention(1024, num_heads=8)\n",
    "\n",
    "        # Enhanced Fully Connected Layers with Dense Connections\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        input = torch.rand(1, *shape)\n",
    "        output = self.q_conv(input)\n",
    "        return int(torch.numel(output) / output.shape[0])\n",
    "\n",
    "    def forward(self, i_input, q_input):\n",
    "        q = self.q_conv(q_input)\n",
    "        q = q.view(q.size(0), -1)\n",
    "\n",
    "        i = self.i_conv(i_input)\n",
    "        i = i.view(i.size(0), -1)\n",
    "\n",
    "        combined = torch.cat((q, i), dim=1)\n",
    "        combined = combined.unsqueeze(1)  # Add sequence dimension\n",
    "\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        # Apply Multi-Head Attention\n",
    "        attn_output = self.multi_head_attn(lstm_out)\n",
    "        context = torch.sum(attn_output, dim=1)  # Sum up the attended output\n",
    "\n",
    "        x = self.fc(context)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = RadioNet(num_classes)\n",
    "    learning_rate = 0.0003\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    return model, optimizer, loss_fn, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312bcf76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T15:37:27.442286Z",
     "iopub.status.busy": "2024-09-11T15:37:27.441915Z",
     "iopub.status.idle": "2024-09-11T15:37:30.403255Z",
     "shell.execute_reply": "2024-09-11T15:37:30.401984Z"
    },
    "papermill": {
     "duration": 2.970093,
     "end_time": "2024-09-11T15:37:30.405972",
     "exception": false,
     "start_time": "2024-09-11T15:37:27.435879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "num_classes = len(selected_modulation_classes)\n",
    "model, optimizer, loss_fn, scheduler = create_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43272ee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T15:37:30.424901Z",
     "iopub.status.busy": "2024-09-11T15:37:30.423235Z",
     "iopub.status.idle": "2024-09-11T15:37:30.929164Z",
     "shell.execute_reply": "2024-09-11T15:37:30.928086Z"
    },
    "papermill": {
     "duration": 0.51872,
     "end_time": "2024-09-11T15:37:30.932015",
     "exception": false,
     "start_time": "2024-09-11T15:37:30.413295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RadioNet(\n",
       "  (q_conv): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1)\n",
       "    (3): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.1)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.1)\n",
       "    (11): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (i_conv): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1)\n",
       "    (3): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.1)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.1)\n",
       "    (11): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(32768, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (multi_head_attn): MultiHeadAttention(\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.1)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.1)\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (10): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (output): Linear(in_features=64, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if multiple GPUs are available and use them if possible\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "    # Wrap the model with DataParallel to parallelize across available GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Set device to CUDA if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8894b661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T15:37:30.949386Z",
     "iopub.status.busy": "2024-09-11T15:37:30.948906Z",
     "iopub.status.idle": "2024-09-11T15:37:44.343986Z",
     "shell.execute_reply": "2024-09-11T15:37:44.342777Z"
    },
    "papermill": {
     "duration": 13.406443,
     "end_time": "2024-09-11T15:37:44.346707",
     "exception": false,
     "start_time": "2024-09-11T15:37:30.940264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of SNRs (from 30 SNR to 22 SNR)\n",
    "N_SNR = 4 \n",
    "\n",
    "# Initialize placeholders for data\n",
    "X_data = None\n",
    "y_data = None\n",
    "\n",
    "# Loop through selected modulation classes\n",
    "for id in selected_classes_id:\n",
    "    # Load data slices based on indices\n",
    "    X_slice = dataset_file['X'][(106496*(id+1) - 4096*N_SNR) : 106496*(id+1)]\n",
    "    y_slice = dataset_file['Y'][(106496*(id+1) - 4096*N_SNR) : 106496*(id+1)]\n",
    "    \n",
    "    # Concatenate the slices to build the dataset\n",
    "    if X_data is not None:\n",
    "        X_data = np.concatenate([X_data, X_slice], axis=0)\n",
    "        y_data = np.concatenate([y_data, y_slice], axis=0)\n",
    "    else:\n",
    "        X_data = X_slice\n",
    "        y_data = y_slice\n",
    "\n",
    "# Reshape the X_data to the required shape (e.g., 32x32 with 2 channels)\n",
    "X_data = X_data.reshape(len(X_data), 32, 32, 2)\n",
    "\n",
    "# Convert y_data to a DataFrame for easier manipulation\n",
    "y_data_df = pd.DataFrame(y_data)\n",
    "\n",
    "# Drop columns where the sum is 0 (i.e., no modulation class data in that column)\n",
    "for column in y_data_df.columns:\n",
    "    if sum(y_data_df[column]) == 0:\n",
    "        y_data_df = y_data_df.drop(columns=[column])\n",
    "\n",
    "# Assign the remaining columns to match the selected modulation classes\n",
    "y_data_df.columns = selected_modulation_classes\n",
    "\n",
    "# Split the dataset into training and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a6dc62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T15:37:44.360035Z",
     "iopub.status.busy": "2024-09-11T15:37:44.359686Z",
     "iopub.status.idle": "2024-09-11T15:37:44.367099Z",
     "shell.execute_reply": "2024-09-11T15:37:44.366040Z"
    },
    "papermill": {
     "duration": 0.016726,
     "end_time": "2024-09-11T15:37:44.369240",
     "exception": false,
     "start_time": "2024-09-11T15:37:44.352514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the custom Dataset class\n",
    "class RadioMLDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float().to(device)\n",
    "        self.y = torch.from_numpy(y.values).float().to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Set up K-fold cross-validation\n",
    "n_splits = 5  # Number of folds\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare the data\n",
    "X = X_train\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "031c9c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T15:37:44.382267Z",
     "iopub.status.busy": "2024-09-11T15:37:44.381930Z",
     "iopub.status.idle": "2024-09-11T15:37:46.040256Z",
     "shell.execute_reply": "2024-09-11T15:37:46.039058Z"
    },
    "papermill": {
     "duration": 1.667437,
     "end_time": "2024-09-11T15:37:46.042558",
     "exception": false,
     "start_time": "2024-09-11T15:37:44.375121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "my_secret = user_secrets.get_secret(\"wandb_api_key\") \n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "453e7031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T15:37:46.056245Z",
     "iopub.status.busy": "2024-09-11T15:37:46.055812Z",
     "iopub.status.idle": "2024-09-11T15:37:46.060594Z",
     "shell.execute_reply": "2024-09-11T15:37:46.059545Z"
    },
    "papermill": {
     "duration": 0.013923,
     "end_time": "2024-09-11T15:37:46.062756",
     "exception": false,
     "start_time": "2024-09-11T15:37:46.048833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# epochs\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4bc60be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T15:37:46.076958Z",
     "iopub.status.busy": "2024-09-11T15:37:46.076007Z",
     "iopub.status.idle": "2024-09-12T00:22:31.265331Z",
     "shell.execute_reply": "2024-09-12T00:22:31.264488Z"
    },
    "papermill": {
     "duration": 31485.199149,
     "end_time": "2024-09-12T00:22:31.267963",
     "exception": false,
     "start_time": "2024-09-11T15:37:46.068814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdevcode03\u001b[0m (\u001b[33mdevcode03-gujarat-technological-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240911_153746-38auhm7w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRadioNetE50_5Fold\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/RadioML\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/RadioML/runs/38auhm7w\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 1, Epoch 1/50, Train Loss: 0.2110, Train Acc: 0.5732, Val Loss: 0.1971, Val Acc: 0.6042\n",
      "Fold 1, Epoch 2/50, Train Loss: 0.1987, Train Acc: 0.6116, Val Loss: 0.1772, Val Acc: 0.6875\n",
      "Fold 1, Epoch 3/50, Train Loss: 0.1631, Train Acc: 0.7604, Val Loss: 0.1508, Val Acc: 0.7961\n",
      "Fold 1, Epoch 4/50, Train Loss: 0.1566, Train Acc: 0.7825, Val Loss: 0.1471, Val Acc: 0.8124\n",
      "Fold 1, Epoch 5/50, Train Loss: 0.1541, Train Acc: 0.7909, Val Loss: 0.1467, Val Acc: 0.8122\n",
      "Fold 1, Epoch 6/50, Train Loss: 0.1516, Train Acc: 0.7970, Val Loss: 0.1427, Val Acc: 0.8180\n",
      "Fold 1, Epoch 7/50, Train Loss: 0.1502, Train Acc: 0.8009, Val Loss: 0.1432, Val Acc: 0.8172\n",
      "Fold 1, Epoch 8/50, Train Loss: 0.1486, Train Acc: 0.8053, Val Loss: 0.1404, Val Acc: 0.8220\n",
      "Fold 1, Epoch 9/50, Train Loss: 0.1456, Train Acc: 0.8080, Val Loss: 0.1380, Val Acc: 0.8218\n",
      "Fold 1, Epoch 10/50, Train Loss: 0.1424, Train Acc: 0.8192, Val Loss: 0.1363, Val Acc: 0.8478\n",
      "Fold 1, Epoch 11/50, Train Loss: 0.1376, Train Acc: 0.8443, Val Loss: 0.1308, Val Acc: 0.8632\n",
      "Fold 1, Epoch 12/50, Train Loss: 0.1335, Train Acc: 0.8552, Val Loss: 0.1298, Val Acc: 0.8594\n",
      "Fold 1, Epoch 13/50, Train Loss: 0.1320, Train Acc: 0.8615, Val Loss: 0.1262, Val Acc: 0.8764\n",
      "Fold 1, Epoch 14/50, Train Loss: 0.1324, Train Acc: 0.8620, Val Loss: 0.1264, Val Acc: 0.8755\n",
      "Fold 1, Epoch 15/50, Train Loss: 0.1300, Train Acc: 0.8677, Val Loss: 0.1243, Val Acc: 0.8805\n",
      "Fold 1, Epoch 16/50, Train Loss: 0.1292, Train Acc: 0.8714, Val Loss: 0.1263, Val Acc: 0.8810\n",
      "Fold 1, Epoch 17/50, Train Loss: 0.1296, Train Acc: 0.8723, Val Loss: 0.1247, Val Acc: 0.8812\n",
      "Fold 1, Epoch 18/50, Train Loss: 0.1297, Train Acc: 0.8713, Val Loss: 0.1235, Val Acc: 0.8852\n",
      "Fold 1, Epoch 19/50, Train Loss: 0.1268, Train Acc: 0.8772, Val Loss: 0.1258, Val Acc: 0.8786\n",
      "Fold 1, Epoch 20/50, Train Loss: 0.1266, Train Acc: 0.8792, Val Loss: 0.1222, Val Acc: 0.8848\n",
      "Fold 1, Epoch 21/50, Train Loss: 0.1245, Train Acc: 0.8862, Val Loss: 0.1187, Val Acc: 0.8993\n",
      "Fold 1, Epoch 22/50, Train Loss: 0.1234, Train Acc: 0.8899, Val Loss: 0.1181, Val Acc: 0.9009\n",
      "Fold 1, Epoch 23/50, Train Loss: 0.1226, Train Acc: 0.8922, Val Loss: 0.1172, Val Acc: 0.9025\n",
      "Fold 1, Epoch 24/50, Train Loss: 0.1199, Train Acc: 0.8975, Val Loss: 0.1156, Val Acc: 0.9010\n",
      "Fold 1, Epoch 25/50, Train Loss: 0.1191, Train Acc: 0.9004, Val Loss: 0.1155, Val Acc: 0.9079\n",
      "Fold 1, Epoch 26/50, Train Loss: 0.1175, Train Acc: 0.9051, Val Loss: 0.1157, Val Acc: 0.9079\n",
      "Fold 1, Epoch 27/50, Train Loss: 0.1171, Train Acc: 0.9077, Val Loss: 0.1156, Val Acc: 0.9053\n",
      "Fold 1, Epoch 28/50, Train Loss: 0.1155, Train Acc: 0.9126, Val Loss: 0.1141, Val Acc: 0.9142\n",
      "Fold 1, Epoch 29/50, Train Loss: 0.1141, Train Acc: 0.9149, Val Loss: 0.1119, Val Acc: 0.9170\n",
      "Fold 1, Epoch 30/50, Train Loss: 0.1135, Train Acc: 0.9161, Val Loss: 0.1119, Val Acc: 0.9168\n",
      "Fold 1, Epoch 31/50, Train Loss: 0.1141, Train Acc: 0.9163, Val Loss: 0.1103, Val Acc: 0.9228\n",
      "Fold 1, Epoch 32/50, Train Loss: 0.1118, Train Acc: 0.9218, Val Loss: 0.1153, Val Acc: 0.9108\n",
      "Fold 1, Epoch 33/50, Train Loss: 0.1113, Train Acc: 0.9248, Val Loss: 0.1098, Val Acc: 0.9231\n",
      "Fold 1, Epoch 34/50, Train Loss: 0.1093, Train Acc: 0.9293, Val Loss: 0.1086, Val Acc: 0.9288\n",
      "Fold 1, Epoch 35/50, Train Loss: 0.1106, Train Acc: 0.9305, Val Loss: 0.1088, Val Acc: 0.9298\n",
      "Fold 1, Epoch 36/50, Train Loss: 0.1075, Train Acc: 0.9371, Val Loss: 0.1075, Val Acc: 0.9333\n",
      "Fold 1, Epoch 37/50, Train Loss: 0.1069, Train Acc: 0.9401, Val Loss: 0.1078, Val Acc: 0.9318\n",
      "Fold 1, Epoch 38/50, Train Loss: 0.1081, Train Acc: 0.9410, Val Loss: 0.1071, Val Acc: 0.9387\n",
      "Fold 1, Epoch 39/50, Train Loss: 0.1073, Train Acc: 0.9409, Val Loss: 0.1083, Val Acc: 0.9311\n",
      "Fold 1, Epoch 40/50, Train Loss: 0.1036, Train Acc: 0.9510, Val Loss: 0.1056, Val Acc: 0.9403\n",
      "Fold 1, Epoch 41/50, Train Loss: 0.1036, Train Acc: 0.9534, Val Loss: 0.1074, Val Acc: 0.9384\n",
      "Fold 1, Epoch 42/50, Train Loss: 0.1036, Train Acc: 0.9544, Val Loss: 0.1075, Val Acc: 0.9353\n",
      "Fold 1, Epoch 43/50, Train Loss: 0.1009, Train Acc: 0.9604, Val Loss: 0.1085, Val Acc: 0.9384\n",
      "Fold 1, Epoch 44/50, Train Loss: 0.1021, Train Acc: 0.9603, Val Loss: 0.1080, Val Acc: 0.9390\n",
      "Fold 1, Epoch 45/50, Train Loss: 0.0985, Train Acc: 0.9672, Val Loss: 0.1081, Val Acc: 0.9417\n",
      "Fold 1, Epoch 46/50, Train Loss: 0.0991, Train Acc: 0.9692, Val Loss: 0.1097, Val Acc: 0.9416\n",
      "Fold 1, Epoch 47/50, Train Loss: 0.0971, Train Acc: 0.9731, Val Loss: 0.1120, Val Acc: 0.9303\n",
      "Fold 1, Epoch 48/50, Train Loss: 0.0963, Train Acc: 0.9759, Val Loss: 0.1084, Val Acc: 0.9360\n",
      "Fold 1, Epoch 49/50, Train Loss: 0.0949, Train Acc: 0.9795, Val Loss: 0.1123, Val Acc: 0.9412\n",
      "Fold 1, Epoch 50/50, Train Loss: 0.0980, Train Acc: 0.9740, Val Loss: 0.1127, Val Acc: 0.9410\n",
      "Fold 2\n",
      "Fold 2, Epoch 1/50, Train Loss: 0.2086, Train Acc: 0.5812, Val Loss: 0.1969, Val Acc: 0.6080\n",
      "Fold 2, Epoch 2/50, Train Loss: 0.1922, Train Acc: 0.6441, Val Loss: 0.1630, Val Acc: 0.7539\n",
      "Fold 2, Epoch 3/50, Train Loss: 0.1621, Train Acc: 0.7642, Val Loss: 0.1549, Val Acc: 0.7989\n",
      "Fold 2, Epoch 4/50, Train Loss: 0.1569, Train Acc: 0.7824, Val Loss: 0.1505, Val Acc: 0.7998\n",
      "Fold 2, Epoch 5/50, Train Loss: 0.1560, Train Acc: 0.7883, Val Loss: 0.1504, Val Acc: 0.8062\n",
      "Fold 2, Epoch 6/50, Train Loss: 0.1526, Train Acc: 0.7978, Val Loss: 0.1506, Val Acc: 0.7925\n",
      "Fold 2, Epoch 7/50, Train Loss: 0.1521, Train Acc: 0.8004, Val Loss: 0.1448, Val Acc: 0.8169\n",
      "Fold 2, Epoch 8/50, Train Loss: 0.1488, Train Acc: 0.8041, Val Loss: 0.1521, Val Acc: 0.7913\n",
      "Fold 2, Epoch 9/50, Train Loss: 0.1427, Train Acc: 0.8214, Val Loss: 0.1308, Val Acc: 0.8589\n",
      "Fold 2, Epoch 10/50, Train Loss: 0.1355, Train Acc: 0.8484, Val Loss: 0.1304, Val Acc: 0.8656\n",
      "Fold 2, Epoch 11/50, Train Loss: 0.1344, Train Acc: 0.8520, Val Loss: 0.1289, Val Acc: 0.8648\n",
      "Fold 2, Epoch 12/50, Train Loss: 0.1327, Train Acc: 0.8604, Val Loss: 0.1266, Val Acc: 0.8744\n",
      "Fold 2, Epoch 13/50, Train Loss: 0.1312, Train Acc: 0.8626, Val Loss: 0.1268, Val Acc: 0.8687\n",
      "Fold 2, Epoch 14/50, Train Loss: 0.1302, Train Acc: 0.8681, Val Loss: 0.1242, Val Acc: 0.8771\n",
      "Fold 2, Epoch 15/50, Train Loss: 0.1280, Train Acc: 0.8728, Val Loss: 0.1272, Val Acc: 0.8749\n",
      "Fold 2, Epoch 16/50, Train Loss: 0.1271, Train Acc: 0.8772, Val Loss: 0.1211, Val Acc: 0.8951\n",
      "Fold 2, Epoch 17/50, Train Loss: 0.1260, Train Acc: 0.8818, Val Loss: 0.1228, Val Acc: 0.8952\n",
      "Fold 2, Epoch 18/50, Train Loss: 0.1237, Train Acc: 0.8873, Val Loss: 0.1188, Val Acc: 0.9025\n",
      "Fold 2, Epoch 19/50, Train Loss: 0.1230, Train Acc: 0.8895, Val Loss: 0.1211, Val Acc: 0.8963\n",
      "Fold 2, Epoch 20/50, Train Loss: 0.1223, Train Acc: 0.8911, Val Loss: 0.1179, Val Acc: 0.9004\n",
      "Fold 2, Epoch 21/50, Train Loss: 0.1203, Train Acc: 0.8974, Val Loss: 0.1177, Val Acc: 0.9027\n",
      "Fold 2, Epoch 22/50, Train Loss: 0.1200, Train Acc: 0.8986, Val Loss: 0.1164, Val Acc: 0.9067\n",
      "Fold 2, Epoch 23/50, Train Loss: 0.1185, Train Acc: 0.9020, Val Loss: 0.1144, Val Acc: 0.9123\n",
      "Fold 2, Epoch 24/50, Train Loss: 0.1182, Train Acc: 0.9031, Val Loss: 0.1135, Val Acc: 0.9131\n",
      "Fold 2, Epoch 25/50, Train Loss: 0.1159, Train Acc: 0.9091, Val Loss: 0.1153, Val Acc: 0.9127\n",
      "Fold 2, Epoch 26/50, Train Loss: 0.1158, Train Acc: 0.9090, Val Loss: 0.1156, Val Acc: 0.9081\n",
      "Fold 2, Epoch 27/50, Train Loss: 0.1158, Train Acc: 0.9104, Val Loss: 0.1176, Val Acc: 0.9017\n",
      "Fold 2, Epoch 28/50, Train Loss: 0.1147, Train Acc: 0.9138, Val Loss: 0.1159, Val Acc: 0.9098\n",
      "Fold 2, Epoch 29/50, Train Loss: 0.1137, Train Acc: 0.9149, Val Loss: 0.1153, Val Acc: 0.9102\n",
      "Fold 2, Epoch 30/50, Train Loss: 0.1125, Train Acc: 0.9184, Val Loss: 0.1112, Val Acc: 0.9198\n",
      "Fold 2, Epoch 31/50, Train Loss: 0.1126, Train Acc: 0.9206, Val Loss: 0.1223, Val Acc: 0.8833\n",
      "Fold 2, Epoch 32/50, Train Loss: 0.1112, Train Acc: 0.9230, Val Loss: 0.1106, Val Acc: 0.9205\n",
      "Fold 2, Epoch 33/50, Train Loss: 0.1100, Train Acc: 0.9271, Val Loss: 0.1121, Val Acc: 0.9260\n",
      "Fold 2, Epoch 34/50, Train Loss: 0.1110, Train Acc: 0.9262, Val Loss: 0.1151, Val Acc: 0.9035\n",
      "Fold 2, Epoch 35/50, Train Loss: 0.1078, Train Acc: 0.9355, Val Loss: 0.1103, Val Acc: 0.9269\n",
      "Fold 2, Epoch 36/50, Train Loss: 0.1060, Train Acc: 0.9420, Val Loss: 0.1098, Val Acc: 0.9298\n",
      "Fold 2, Epoch 37/50, Train Loss: 0.1075, Train Acc: 0.9416, Val Loss: 0.1093, Val Acc: 0.9280\n",
      "Fold 2, Epoch 38/50, Train Loss: 0.1043, Train Acc: 0.9481, Val Loss: 0.1108, Val Acc: 0.9226\n",
      "Fold 2, Epoch 39/50, Train Loss: 0.1032, Train Acc: 0.9520, Val Loss: 0.1107, Val Acc: 0.9312\n",
      "Fold 2, Epoch 40/50, Train Loss: 0.1019, Train Acc: 0.9568, Val Loss: 0.1101, Val Acc: 0.9331\n",
      "Fold 2, Epoch 41/50, Train Loss: 0.1007, Train Acc: 0.9612, Val Loss: 0.1091, Val Acc: 0.9325\n",
      "Fold 2, Epoch 42/50, Train Loss: 0.1008, Train Acc: 0.9625, Val Loss: 0.1161, Val Acc: 0.9216\n",
      "Fold 2, Epoch 43/50, Train Loss: 0.0978, Train Acc: 0.9704, Val Loss: 0.1101, Val Acc: 0.9397\n",
      "Fold 2, Epoch 44/50, Train Loss: 0.0983, Train Acc: 0.9711, Val Loss: 0.1223, Val Acc: 0.9292\n",
      "Fold 2, Epoch 45/50, Train Loss: 0.0974, Train Acc: 0.9728, Val Loss: 0.1122, Val Acc: 0.9395\n",
      "Fold 2, Epoch 46/50, Train Loss: 0.0952, Train Acc: 0.9782, Val Loss: 0.1121, Val Acc: 0.9351\n",
      "Fold 2, Epoch 47/50, Train Loss: 0.0966, Train Acc: 0.9770, Val Loss: 0.1122, Val Acc: 0.9393\n",
      "Fold 2, Epoch 48/50, Train Loss: 0.0940, Train Acc: 0.9821, Val Loss: 0.1117, Val Acc: 0.9405\n",
      "Fold 2, Epoch 49/50, Train Loss: 0.0940, Train Acc: 0.9832, Val Loss: 0.1200, Val Acc: 0.9214\n",
      "Fold 2, Epoch 50/50, Train Loss: 0.0942, Train Acc: 0.9836, Val Loss: 0.1185, Val Acc: 0.9321\n",
      "Fold 3\n",
      "Fold 3, Epoch 1/50, Train Loss: 0.2090, Train Acc: 0.5795, Val Loss: 0.2049, Val Acc: 0.5914\n",
      "Fold 3, Epoch 2/50, Train Loss: 0.1802, Train Acc: 0.6964, Val Loss: 0.1571, Val Acc: 0.7836\n",
      "Fold 3, Epoch 3/50, Train Loss: 0.1590, Train Acc: 0.7760, Val Loss: 0.1501, Val Acc: 0.8046\n",
      "Fold 3, Epoch 4/50, Train Loss: 0.1568, Train Acc: 0.7825, Val Loss: 0.1466, Val Acc: 0.8177\n",
      "Fold 3, Epoch 5/50, Train Loss: 0.1547, Train Acc: 0.7893, Val Loss: 0.1539, Val Acc: 0.7983\n",
      "Fold 3, Epoch 6/50, Train Loss: 0.1536, Train Acc: 0.7919, Val Loss: 0.1465, Val Acc: 0.8133\n",
      "Fold 3, Epoch 7/50, Train Loss: 0.1507, Train Acc: 0.7987, Val Loss: 0.1447, Val Acc: 0.8231\n",
      "Fold 3, Epoch 8/50, Train Loss: 0.1499, Train Acc: 0.7998, Val Loss: 0.1438, Val Acc: 0.8223\n",
      "Fold 3, Epoch 9/50, Train Loss: 0.1456, Train Acc: 0.8068, Val Loss: 0.1351, Val Acc: 0.8472\n",
      "Fold 3, Epoch 10/50, Train Loss: 0.1394, Train Acc: 0.8340, Val Loss: 0.1293, Val Acc: 0.8696\n",
      "Fold 3, Epoch 11/50, Train Loss: 0.1364, Train Acc: 0.8463, Val Loss: 0.1297, Val Acc: 0.8623\n",
      "Fold 3, Epoch 12/50, Train Loss: 0.1336, Train Acc: 0.8535, Val Loss: 0.1281, Val Acc: 0.8747\n",
      "Fold 3, Epoch 13/50, Train Loss: 0.1321, Train Acc: 0.8599, Val Loss: 0.1307, Val Acc: 0.8655\n",
      "Fold 3, Epoch 14/50, Train Loss: 0.1350, Train Acc: 0.8554, Val Loss: 0.1240, Val Acc: 0.8841\n",
      "Fold 3, Epoch 15/50, Train Loss: 0.1313, Train Acc: 0.8646, Val Loss: 0.1288, Val Acc: 0.8757\n",
      "Fold 3, Epoch 16/50, Train Loss: 0.1294, Train Acc: 0.8697, Val Loss: 0.1238, Val Acc: 0.8860\n",
      "Fold 3, Epoch 17/50, Train Loss: 0.1292, Train Acc: 0.8715, Val Loss: 0.1226, Val Acc: 0.8924\n",
      "Fold 3, Epoch 18/50, Train Loss: 0.1272, Train Acc: 0.8754, Val Loss: 0.1249, Val Acc: 0.8847\n",
      "Fold 3, Epoch 19/50, Train Loss: 0.1277, Train Acc: 0.8757, Val Loss: 0.1247, Val Acc: 0.8845\n",
      "Fold 3, Epoch 20/50, Train Loss: 0.1242, Train Acc: 0.8847, Val Loss: 0.1186, Val Acc: 0.8996\n",
      "Fold 3, Epoch 21/50, Train Loss: 0.1239, Train Acc: 0.8870, Val Loss: 0.1184, Val Acc: 0.9026\n",
      "Fold 3, Epoch 22/50, Train Loss: 0.1215, Train Acc: 0.8935, Val Loss: 0.1168, Val Acc: 0.9075\n",
      "Fold 3, Epoch 23/50, Train Loss: 0.1201, Train Acc: 0.8986, Val Loss: 0.1147, Val Acc: 0.9092\n",
      "Fold 3, Epoch 24/50, Train Loss: 0.1207, Train Acc: 0.8975, Val Loss: 0.1159, Val Acc: 0.9072\n",
      "Fold 3, Epoch 25/50, Train Loss: 0.1170, Train Acc: 0.9066, Val Loss: 0.1156, Val Acc: 0.9124\n",
      "Fold 3, Epoch 26/50, Train Loss: 0.1164, Train Acc: 0.9069, Val Loss: 0.1153, Val Acc: 0.9114\n",
      "Fold 3, Epoch 27/50, Train Loss: 0.1154, Train Acc: 0.9095, Val Loss: 0.1130, Val Acc: 0.9124\n",
      "Fold 3, Epoch 28/50, Train Loss: 0.1178, Train Acc: 0.9101, Val Loss: 0.1126, Val Acc: 0.9195\n",
      "Fold 3, Epoch 29/50, Train Loss: 0.1133, Train Acc: 0.9174, Val Loss: 0.1129, Val Acc: 0.9173\n",
      "Fold 3, Epoch 30/50, Train Loss: 0.1131, Train Acc: 0.9178, Val Loss: 0.1132, Val Acc: 0.9159\n",
      "Fold 3, Epoch 31/50, Train Loss: 0.1123, Train Acc: 0.9204, Val Loss: 0.1106, Val Acc: 0.9241\n",
      "Fold 3, Epoch 32/50, Train Loss: 0.1122, Train Acc: 0.9245, Val Loss: 0.1132, Val Acc: 0.9190\n",
      "Fold 3, Epoch 33/50, Train Loss: 0.1115, Train Acc: 0.9268, Val Loss: 0.1102, Val Acc: 0.9217\n",
      "Fold 3, Epoch 34/50, Train Loss: 0.1092, Train Acc: 0.9310, Val Loss: 0.1119, Val Acc: 0.9227\n",
      "Fold 3, Epoch 35/50, Train Loss: 0.1077, Train Acc: 0.9365, Val Loss: 0.1101, Val Acc: 0.9292\n",
      "Fold 3, Epoch 36/50, Train Loss: 0.1074, Train Acc: 0.9385, Val Loss: 0.1101, Val Acc: 0.9207\n",
      "Fold 3, Epoch 37/50, Train Loss: 0.1052, Train Acc: 0.9451, Val Loss: 0.1095, Val Acc: 0.9325\n",
      "Fold 3, Epoch 38/50, Train Loss: 0.1051, Train Acc: 0.9472, Val Loss: 0.1103, Val Acc: 0.9300\n",
      "Fold 3, Epoch 39/50, Train Loss: 0.1042, Train Acc: 0.9498, Val Loss: 0.1134, Val Acc: 0.9237\n",
      "Fold 3, Epoch 40/50, Train Loss: 0.1027, Train Acc: 0.9553, Val Loss: 0.1094, Val Acc: 0.9372\n",
      "Fold 3, Epoch 41/50, Train Loss: 0.1009, Train Acc: 0.9603, Val Loss: 0.1103, Val Acc: 0.9353\n",
      "Fold 3, Epoch 42/50, Train Loss: 0.1017, Train Acc: 0.9612, Val Loss: 0.1085, Val Acc: 0.9372\n",
      "Fold 3, Epoch 43/50, Train Loss: 0.0992, Train Acc: 0.9664, Val Loss: 0.1084, Val Acc: 0.9351\n",
      "Fold 3, Epoch 44/50, Train Loss: 0.0983, Train Acc: 0.9693, Val Loss: 0.1101, Val Acc: 0.9368\n",
      "Fold 3, Epoch 45/50, Train Loss: 0.0995, Train Acc: 0.9691, Val Loss: 0.1106, Val Acc: 0.9401\n",
      "Fold 3, Epoch 46/50, Train Loss: 0.0966, Train Acc: 0.9742, Val Loss: 0.1143, Val Acc: 0.9367\n",
      "Fold 3, Epoch 47/50, Train Loss: 0.0964, Train Acc: 0.9758, Val Loss: 0.1127, Val Acc: 0.9349\n",
      "Fold 3, Epoch 48/50, Train Loss: 0.0946, Train Acc: 0.9797, Val Loss: 0.1155, Val Acc: 0.9339\n",
      "Fold 3, Epoch 49/50, Train Loss: 0.0957, Train Acc: 0.9790, Val Loss: 0.1130, Val Acc: 0.9369\n",
      "Fold 3, Epoch 50/50, Train Loss: 0.0952, Train Acc: 0.9809, Val Loss: 0.1195, Val Acc: 0.9370\n",
      "Fold 4\n",
      "Fold 4, Epoch 1/50, Train Loss: 0.2080, Train Acc: 0.5855, Val Loss: 0.1965, Val Acc: 0.6047\n",
      "Fold 4, Epoch 2/50, Train Loss: 0.1756, Train Acc: 0.7122, Val Loss: 0.1504, Val Acc: 0.8070\n",
      "Fold 4, Epoch 3/50, Train Loss: 0.1597, Train Acc: 0.7733, Val Loss: 0.1467, Val Acc: 0.8182\n",
      "Fold 4, Epoch 4/50, Train Loss: 0.1552, Train Acc: 0.7886, Val Loss: 0.1468, Val Acc: 0.8184\n",
      "Fold 4, Epoch 5/50, Train Loss: 0.1535, Train Acc: 0.7933, Val Loss: 0.1461, Val Acc: 0.8204\n",
      "Fold 4, Epoch 6/50, Train Loss: 0.1520, Train Acc: 0.7973, Val Loss: 0.1447, Val Acc: 0.8124\n",
      "Fold 4, Epoch 7/50, Train Loss: 0.1497, Train Acc: 0.8014, Val Loss: 0.1413, Val Acc: 0.8205\n",
      "Fold 4, Epoch 8/50, Train Loss: 0.1452, Train Acc: 0.8086, Val Loss: 0.1335, Val Acc: 0.8479\n",
      "Fold 4, Epoch 9/50, Train Loss: 0.1385, Train Acc: 0.8380, Val Loss: 0.1299, Val Acc: 0.8695\n",
      "Fold 4, Epoch 10/50, Train Loss: 0.1349, Train Acc: 0.8505, Val Loss: 0.1326, Val Acc: 0.8626\n",
      "Fold 4, Epoch 11/50, Train Loss: 0.1345, Train Acc: 0.8539, Val Loss: 0.1246, Val Acc: 0.8827\n",
      "Fold 4, Epoch 12/50, Train Loss: 0.1323, Train Acc: 0.8585, Val Loss: 0.1235, Val Acc: 0.8859\n",
      "Fold 4, Epoch 13/50, Train Loss: 0.1315, Train Acc: 0.8644, Val Loss: 0.1234, Val Acc: 0.8902\n",
      "Fold 4, Epoch 14/50, Train Loss: 0.1298, Train Acc: 0.8681, Val Loss: 0.1246, Val Acc: 0.8849\n",
      "Fold 4, Epoch 15/50, Train Loss: 0.1295, Train Acc: 0.8701, Val Loss: 0.1238, Val Acc: 0.8827\n",
      "Fold 4, Epoch 16/50, Train Loss: 0.1272, Train Acc: 0.8759, Val Loss: 0.1225, Val Acc: 0.8872\n",
      "Fold 4, Epoch 17/50, Train Loss: 0.1264, Train Acc: 0.8784, Val Loss: 0.1195, Val Acc: 0.8991\n",
      "Fold 4, Epoch 18/50, Train Loss: 0.1249, Train Acc: 0.8843, Val Loss: 0.1185, Val Acc: 0.8992\n",
      "Fold 4, Epoch 19/50, Train Loss: 0.1234, Train Acc: 0.8867, Val Loss: 0.1174, Val Acc: 0.9031\n",
      "Fold 4, Epoch 20/50, Train Loss: 0.1214, Train Acc: 0.8938, Val Loss: 0.1147, Val Acc: 0.9128\n",
      "Fold 4, Epoch 21/50, Train Loss: 0.1199, Train Acc: 0.8981, Val Loss: 0.1135, Val Acc: 0.9156\n",
      "Fold 4, Epoch 22/50, Train Loss: 0.1184, Train Acc: 0.9021, Val Loss: 0.1133, Val Acc: 0.9166\n",
      "Fold 4, Epoch 23/50, Train Loss: 0.1172, Train Acc: 0.9054, Val Loss: 0.1123, Val Acc: 0.9177\n",
      "Fold 4, Epoch 24/50, Train Loss: 0.1168, Train Acc: 0.9076, Val Loss: 0.1122, Val Acc: 0.9177\n",
      "Fold 4, Epoch 25/50, Train Loss: 0.1152, Train Acc: 0.9113, Val Loss: 0.1120, Val Acc: 0.9202\n",
      "Fold 4, Epoch 26/50, Train Loss: 0.1151, Train Acc: 0.9136, Val Loss: 0.1125, Val Acc: 0.9194\n",
      "Fold 4, Epoch 27/50, Train Loss: 0.1132, Train Acc: 0.9172, Val Loss: 0.1114, Val Acc: 0.9205\n",
      "Fold 4, Epoch 28/50, Train Loss: 0.1126, Train Acc: 0.9192, Val Loss: 0.1164, Val Acc: 0.9117\n",
      "Fold 4, Epoch 29/50, Train Loss: 0.1133, Train Acc: 0.9199, Val Loss: 0.1122, Val Acc: 0.9206\n",
      "Fold 4, Epoch 30/50, Train Loss: 0.1113, Train Acc: 0.9231, Val Loss: 0.1161, Val Acc: 0.9080\n",
      "Fold 4, Epoch 31/50, Train Loss: 0.1104, Train Acc: 0.9254, Val Loss: 0.1139, Val Acc: 0.9158\n",
      "Fold 4, Epoch 32/50, Train Loss: 0.1109, Train Acc: 0.9269, Val Loss: 0.1084, Val Acc: 0.9290\n",
      "Fold 4, Epoch 33/50, Train Loss: 0.1079, Train Acc: 0.9349, Val Loss: 0.1107, Val Acc: 0.9243\n",
      "Fold 4, Epoch 34/50, Train Loss: 0.1085, Train Acc: 0.9365, Val Loss: 0.1089, Val Acc: 0.9282\n",
      "Fold 4, Epoch 35/50, Train Loss: 0.1053, Train Acc: 0.9441, Val Loss: 0.1075, Val Acc: 0.9348\n",
      "Fold 4, Epoch 36/50, Train Loss: 0.1043, Train Acc: 0.9484, Val Loss: 0.1071, Val Acc: 0.9381\n",
      "Fold 4, Epoch 37/50, Train Loss: 0.1067, Train Acc: 0.9477, Val Loss: 0.1071, Val Acc: 0.9360\n",
      "Fold 4, Epoch 38/50, Train Loss: 0.1012, Train Acc: 0.9590, Val Loss: 0.1086, Val Acc: 0.9375\n",
      "Fold 4, Epoch 39/50, Train Loss: 0.1016, Train Acc: 0.9601, Val Loss: 0.1112, Val Acc: 0.9359\n",
      "Fold 4, Epoch 40/50, Train Loss: 0.0987, Train Acc: 0.9679, Val Loss: 0.1156, Val Acc: 0.9376\n",
      "Fold 4, Epoch 41/50, Train Loss: 0.0989, Train Acc: 0.9690, Val Loss: 0.1080, Val Acc: 0.9426\n",
      "Fold 4, Epoch 42/50, Train Loss: 0.0979, Train Acc: 0.9739, Val Loss: 0.1154, Val Acc: 0.9333\n",
      "Fold 4, Epoch 43/50, Train Loss: 0.0968, Train Acc: 0.9758, Val Loss: 0.1132, Val Acc: 0.9326\n",
      "Fold 4, Epoch 44/50, Train Loss: 0.0946, Train Acc: 0.9797, Val Loss: 0.1258, Val Acc: 0.9172\n",
      "Fold 4, Epoch 45/50, Train Loss: 0.0945, Train Acc: 0.9806, Val Loss: 0.1189, Val Acc: 0.9213\n",
      "Fold 4, Epoch 46/50, Train Loss: 0.0960, Train Acc: 0.9786, Val Loss: 0.1136, Val Acc: 0.9398\n",
      "Fold 4, Epoch 47/50, Train Loss: 0.0934, Train Acc: 0.9843, Val Loss: 0.1117, Val Acc: 0.9438\n",
      "Fold 4, Epoch 48/50, Train Loss: 0.0893, Train Acc: 0.9934, Val Loss: 0.1189, Val Acc: 0.9464\n",
      "Fold 4, Epoch 49/50, Train Loss: 0.0883, Train Acc: 0.9957, Val Loss: 0.1187, Val Acc: 0.9463\n",
      "Fold 4, Epoch 50/50, Train Loss: 0.0882, Train Acc: 0.9963, Val Loss: 0.1183, Val Acc: 0.9468\n",
      "Fold 5\n",
      "Fold 5, Epoch 1/50, Train Loss: 0.2095, Train Acc: 0.5814, Val Loss: 0.2002, Val Acc: 0.6052\n",
      "Fold 5, Epoch 2/50, Train Loss: 0.1779, Train Acc: 0.7066, Val Loss: 0.1637, Val Acc: 0.7527\n",
      "Fold 5, Epoch 3/50, Train Loss: 0.1596, Train Acc: 0.7738, Val Loss: 0.1551, Val Acc: 0.7870\n",
      "Fold 5, Epoch 4/50, Train Loss: 0.1565, Train Acc: 0.7856, Val Loss: 0.1495, Val Acc: 0.8154\n",
      "Fold 5, Epoch 5/50, Train Loss: 0.1538, Train Acc: 0.7923, Val Loss: 0.1467, Val Acc: 0.8187\n",
      "Fold 5, Epoch 6/50, Train Loss: 0.1501, Train Acc: 0.7977, Val Loss: 0.1428, Val Acc: 0.8074\n",
      "Fold 5, Epoch 7/50, Train Loss: 0.1405, Train Acc: 0.8295, Val Loss: 0.1319, Val Acc: 0.8472\n",
      "Fold 5, Epoch 8/50, Train Loss: 0.1361, Train Acc: 0.8466, Val Loss: 0.1270, Val Acc: 0.8685\n",
      "Fold 5, Epoch 9/50, Train Loss: 0.1339, Train Acc: 0.8551, Val Loss: 0.1315, Val Acc: 0.8606\n",
      "Fold 5, Epoch 10/50, Train Loss: 0.1373, Train Acc: 0.8493, Val Loss: 0.1287, Val Acc: 0.8682\n",
      "Fold 5, Epoch 11/50, Train Loss: 0.1314, Train Acc: 0.8622, Val Loss: 0.1239, Val Acc: 0.8827\n",
      "Fold 5, Epoch 12/50, Train Loss: 0.1314, Train Acc: 0.8657, Val Loss: 0.1253, Val Acc: 0.8778\n",
      "Fold 5, Epoch 13/50, Train Loss: 0.1297, Train Acc: 0.8695, Val Loss: 0.1265, Val Acc: 0.8709\n",
      "Fold 5, Epoch 14/50, Train Loss: 0.1276, Train Acc: 0.8752, Val Loss: 0.1219, Val Acc: 0.8866\n",
      "Fold 5, Epoch 15/50, Train Loss: 0.1260, Train Acc: 0.8806, Val Loss: 0.1197, Val Acc: 0.8995\n",
      "Fold 5, Epoch 16/50, Train Loss: 0.1245, Train Acc: 0.8839, Val Loss: 0.1184, Val Acc: 0.8970\n",
      "Fold 5, Epoch 17/50, Train Loss: 0.1220, Train Acc: 0.8902, Val Loss: 0.1158, Val Acc: 0.9061\n",
      "Fold 5, Epoch 18/50, Train Loss: 0.1200, Train Acc: 0.8959, Val Loss: 0.1158, Val Acc: 0.9035\n",
      "Fold 5, Epoch 19/50, Train Loss: 0.1201, Train Acc: 0.8970, Val Loss: 0.1146, Val Acc: 0.9110\n",
      "Fold 5, Epoch 20/50, Train Loss: 0.1183, Train Acc: 0.9014, Val Loss: 0.1134, Val Acc: 0.9137\n",
      "Fold 5, Epoch 21/50, Train Loss: 0.1174, Train Acc: 0.9045, Val Loss: 0.1154, Val Acc: 0.9124\n",
      "Fold 5, Epoch 22/50, Train Loss: 0.1174, Train Acc: 0.9049, Val Loss: 0.1121, Val Acc: 0.9164\n",
      "Fold 5, Epoch 23/50, Train Loss: 0.1153, Train Acc: 0.9100, Val Loss: 0.1135, Val Acc: 0.9143\n",
      "Fold 5, Epoch 24/50, Train Loss: 0.1161, Train Acc: 0.9105, Val Loss: 0.1131, Val Acc: 0.9131\n",
      "Fold 5, Epoch 25/50, Train Loss: 0.1154, Train Acc: 0.9113, Val Loss: 0.1125, Val Acc: 0.9161\n",
      "Fold 5, Epoch 26/50, Train Loss: 0.1132, Train Acc: 0.9160, Val Loss: 0.1116, Val Acc: 0.9184\n",
      "Fold 5, Epoch 27/50, Train Loss: 0.1125, Train Acc: 0.9207, Val Loss: 0.1099, Val Acc: 0.9238\n",
      "Fold 5, Epoch 28/50, Train Loss: 0.1127, Train Acc: 0.9233, Val Loss: 0.1098, Val Acc: 0.9253\n",
      "Fold 5, Epoch 29/50, Train Loss: 0.1105, Train Acc: 0.9275, Val Loss: 0.1080, Val Acc: 0.9310\n",
      "Fold 5, Epoch 30/50, Train Loss: 0.1090, Train Acc: 0.9343, Val Loss: 0.1105, Val Acc: 0.9260\n",
      "Fold 5, Epoch 31/50, Train Loss: 0.1083, Train Acc: 0.9365, Val Loss: 0.1154, Val Acc: 0.9202\n",
      "Fold 5, Epoch 32/50, Train Loss: 0.1069, Train Acc: 0.9427, Val Loss: 0.1104, Val Acc: 0.9281\n",
      "Fold 5, Epoch 33/50, Train Loss: 0.1054, Train Acc: 0.9467, Val Loss: 0.1097, Val Acc: 0.9383\n",
      "Fold 5, Epoch 34/50, Train Loss: 0.1058, Train Acc: 0.9477, Val Loss: 0.1057, Val Acc: 0.9427\n",
      "Fold 5, Epoch 35/50, Train Loss: 0.1051, Train Acc: 0.9501, Val Loss: 0.1073, Val Acc: 0.9392\n",
      "Fold 5, Epoch 36/50, Train Loss: 0.1018, Train Acc: 0.9592, Val Loss: 0.1066, Val Acc: 0.9420\n",
      "Fold 5, Epoch 37/50, Train Loss: 0.1006, Train Acc: 0.9615, Val Loss: 0.1095, Val Acc: 0.9401\n",
      "Fold 5, Epoch 38/50, Train Loss: 0.1023, Train Acc: 0.9600, Val Loss: 0.1073, Val Acc: 0.9446\n",
      "Fold 5, Epoch 39/50, Train Loss: 0.0991, Train Acc: 0.9674, Val Loss: 0.1091, Val Acc: 0.9399\n",
      "Fold 5, Epoch 40/50, Train Loss: 0.0977, Train Acc: 0.9712, Val Loss: 0.1064, Val Acc: 0.9444\n",
      "Fold 5, Epoch 41/50, Train Loss: 0.0978, Train Acc: 0.9721, Val Loss: 0.1097, Val Acc: 0.9411\n",
      "Fold 5, Epoch 42/50, Train Loss: 0.0991, Train Acc: 0.9722, Val Loss: 0.1116, Val Acc: 0.9311\n",
      "Fold 5, Epoch 43/50, Train Loss: 0.0951, Train Acc: 0.9783, Val Loss: 0.1108, Val Acc: 0.9415\n",
      "Fold 5, Epoch 44/50, Train Loss: 0.0978, Train Acc: 0.9755, Val Loss: 0.1136, Val Acc: 0.9390\n",
      "Fold 5, Epoch 45/50, Train Loss: 0.0934, Train Acc: 0.9828, Val Loss: 0.1149, Val Acc: 0.9421\n",
      "Fold 5, Epoch 46/50, Train Loss: 0.0900, Train Acc: 0.9922, Val Loss: 0.1140, Val Acc: 0.9489\n",
      "Fold 5, Epoch 47/50, Train Loss: 0.0887, Train Acc: 0.9951, Val Loss: 0.1158, Val Acc: 0.9474\n",
      "Fold 5, Epoch 48/50, Train Loss: 0.0885, Train Acc: 0.9956, Val Loss: 0.1155, Val Acc: 0.9480\n",
      "Fold 5, Epoch 49/50, Train Loss: 0.0883, Train Acc: 0.9962, Val Loss: 0.1178, Val Acc: 0.9478\n",
      "Fold 5, Epoch 50/50, Train Loss: 0.0881, Train Acc: 0.9966, Val Loss: 0.1237, Val Acc: 0.9456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  fold_1_best_val_acc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         fold_1_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_1_learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     fold_1_train_acc ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    fold_1_train_loss ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       fold_1_val_acc ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      fold_1_val_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  fold_2_best_val_acc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         fold_2_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_2_learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     fold_2_train_acc ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    fold_2_train_loss ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       fold_2_val_acc ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      fold_2_val_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  fold_3_best_val_acc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         fold_3_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_3_learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     fold_3_train_acc ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    fold_3_train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       fold_3_val_acc ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      fold_3_val_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  fold_4_best_val_acc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         fold_4_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_4_learning_rate ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     fold_4_train_acc ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    fold_4_train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       fold_4_val_acc ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      fold_4_val_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  fold_5_best_val_acc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         fold_5_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_5_learning_rate ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     fold_5_train_acc ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    fold_5_train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       fold_5_val_acc ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      fold_5_val_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  fold_1_best_val_acc 0.94168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         fold_1_epoch 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_1_learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     fold_1_train_acc 0.97399\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    fold_1_train_loss 0.09801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       fold_1_val_acc 0.94097\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      fold_1_val_loss 0.1127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  fold_2_best_val_acc 0.94049\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         fold_2_epoch 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_2_learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     fold_2_train_acc 0.9836\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    fold_2_train_loss 0.09422\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       fold_2_val_acc 0.93215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      fold_2_val_loss 0.11853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  fold_3_best_val_acc 0.94006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         fold_3_epoch 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_3_learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     fold_3_train_acc 0.98085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    fold_3_train_loss 0.09522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       fold_3_val_acc 0.93696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      fold_3_val_loss 0.11952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  fold_4_best_val_acc 0.94683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         fold_4_epoch 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_4_learning_rate 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     fold_4_train_acc 0.9963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    fold_4_train_loss 0.08824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       fold_4_val_acc 0.94683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      fold_4_val_loss 0.11825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  fold_5_best_val_acc 0.94893\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         fold_5_epoch 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_5_learning_rate 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     fold_5_train_acc 0.99655\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    fold_5_train_loss 0.08812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       fold_5_val_acc 0.94564\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      fold_5_val_loss 0.12372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mRadioNetE50_5Fold\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/RadioML/runs/38auhm7w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/RadioML\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240911_153746-38auhm7w/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
     ]
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project=\"RadioML\", name=\"RadioNetE50_5Fold\")\n",
    "\n",
    "# Perform K-fold cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X), 1):\n",
    "    print(f\"Fold {fold}\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Create Dataset objects\n",
    "    train_dataset = RadioMLDataset(X_train, y_train)\n",
    "    val_dataset = RadioMLDataset(X_val, y_val)\n",
    "    \n",
    "    # Create DataLoader objects\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model, optimizer, loss function, and scheduler\n",
    "    model = RadioNet(num_classes).to(device)  # Assuming RadioNet is your model class\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    \n",
    "    # Log model architecture\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience = 20\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            i_input = batch_X[:, :, :, 0].unsqueeze(1)\n",
    "            q_input = batch_X[:, :, :, 1].unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(i_input, q_input)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(1) == batch_y.argmax(1)).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / len(train_dataset)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                i_input = batch_X[:, :, :, 0].unsqueeze(1)\n",
    "                q_input = batch_X[:, :, :, 1].unsqueeze(1)\n",
    "                outputs = model(i_input, q_input)\n",
    "                val_loss += loss_fn(outputs, batch_y).item()\n",
    "                val_correct += (outputs.argmax(1) == batch_y.argmax(1)).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / len(val_dataset)\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            f\"fold_{fold}_epoch\": epoch + 1,\n",
    "            f\"fold_{fold}_train_loss\": train_loss,\n",
    "            f\"fold_{fold}_train_acc\": train_acc,\n",
    "            f\"fold_{fold}_val_loss\": val_loss,\n",
    "            f\"fold_{fold}_val_acc\": val_acc,\n",
    "            f\"fold_{fold}_learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Fold {fold}, Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping and model checkpointing\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"checkpoint_fold_{fold}.pth\")\n",
    "            wandb.save(f\"checkpoint_fold_{fold}.pth\")\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve == patience:\n",
    "                print(f\"Early stopping on fold {fold}\")\n",
    "                break\n",
    "    \n",
    "    # Log best validation accuracy for this fold\n",
    "    wandb.log({f\"fold_{fold}_best_val_acc\": best_val_acc})\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87710ea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T00:22:31.328605Z",
     "iopub.status.busy": "2024-09-12T00:22:31.328188Z",
     "iopub.status.idle": "2024-09-12T00:22:31.335641Z",
     "shell.execute_reply": "2024-09-12T00:22:31.334660Z"
    },
    "papermill": {
     "duration": 0.041225,
     "end_time": "2024-09-12T00:22:31.337941",
     "exception": false,
     "start_time": "2024-09-12T00:22:31.296716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initialize wandb\n",
    "# wandb.init(project=\"RadioML\", name=\"RadioNetE50\")\n",
    "\n",
    "# # Log model architecture\n",
    "# wandb.watch(model)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     # Set model to training mode\n",
    "#     model.train()\n",
    "#     train_loss, train_correct = 0, 0\n",
    "#     # Training loop\n",
    "#     for batch_X, batch_y in train_loader:\n",
    "#         # Split the input into I and Q components\n",
    "#         i_input = batch_X[:, :, :, 0].unsqueeze(1)  # I component\n",
    "#         q_input = batch_X[:, :, :, 1].unsqueeze(1)  # Q component\n",
    "#         # Zero out the gradients\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass through the model\n",
    "#         outputs = model(i_input, q_input)\n",
    "#         # Compute loss\n",
    "#         loss = loss_fn(outputs, batch_y)\n",
    "#         # Backpropagation\n",
    "#         loss.backward()\n",
    "#         # Update model parameters\n",
    "#         optimizer.step()\n",
    "#         # Accumulate training loss and correct predictions\n",
    "#         train_loss += loss.item()\n",
    "#         train_correct += (outputs.argmax(1) == batch_y.argmax(1)).sum().item()\n",
    "#     # Compute average training loss and accuracy\n",
    "#     train_loss /= len(train_loader)\n",
    "#     train_acc = train_correct / len(train_dataset)\n",
    "#     # Validation loop (without gradient updates)\n",
    "#     model.eval()\n",
    "#     val_loss, val_correct = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "#             # Split the input into I and Q components\n",
    "#             i_input = batch_X[:, :, :, 0].unsqueeze(1)  # I component\n",
    "#             q_input = batch_X[:, :, :, 1].unsqueeze(1)  # Q component\n",
    "#             # Forward pass through the model\n",
    "#             outputs = model(i_input, q_input)\n",
    "#             # Compute validation loss\n",
    "#             val_loss += loss_fn(outputs, batch_y).item()\n",
    "#             # Accumulate correct predictions\n",
    "#             val_correct += (outputs.argmax(1) == batch_y.argmax(1)).sum().item()\n",
    "#     # Compute average validation loss and accuracy\n",
    "#     val_loss /= len(test_loader)\n",
    "#     val_acc = val_correct / len(test_dataset)\n",
    "#     # Save loss and accuracy for later plotting\n",
    "#     train_losses.append(train_loss)\n",
    "#     train_accs.append(train_acc)\n",
    "#     val_losses.append(val_loss)\n",
    "#     val_accs.append(val_acc)\n",
    "\n",
    "#     # Log metrics to wandb\n",
    "#     wandb.log({\n",
    "#         \"epoch\": epoch + 1,\n",
    "#         \"train_loss\": train_loss,\n",
    "#         \"train_acc\": train_acc,\n",
    "#         \"val_loss\": val_loss,\n",
    "#         \"val_acc\": val_acc,\n",
    "#         \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "#     })\n",
    "\n",
    "#     # Print progress for this epoch\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "#           f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "#     # Step the learning rate scheduler based on validation loss\n",
    "#     scheduler.step(val_loss)\n",
    "#     # Early stopping and model checkpointing\n",
    "#     if val_acc > best_acc:\n",
    "#         best_acc = val_acc\n",
    "#         torch.save(model.state_dict(), path_checkpoint)  # Save the model checkpoint\n",
    "#         wandb.save(path_checkpoint)  # Save the model checkpoint to wandb\n",
    "#         no_improve = 0  # Reset no improvement counter\n",
    "#     else:\n",
    "#         no_improve += 1\n",
    "#         if no_improve == patience:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "# # Finish the wandb run\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af73b896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T00:22:31.396723Z",
     "iopub.status.busy": "2024-09-12T00:22:31.396337Z",
     "iopub.status.idle": "2024-09-12T00:22:31.400889Z",
     "shell.execute_reply": "2024-09-12T00:22:31.399982Z"
    },
    "papermill": {
     "duration": 0.036396,
     "end_time": "2024-09-12T00:22:31.403007",
     "exception": false,
     "start_time": "2024-09-12T00:22:31.366611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(train_accs, label='Train Accuracy')\n",
    "# plt.plot(val_accs, label='Validation Accuracy')\n",
    "# plt.legend()\n",
    "# plt.title('Accuracy')\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(train_losses, label='Train Loss')\n",
    "# plt.plot(val_losses, label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "480c1d4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T00:22:31.461976Z",
     "iopub.status.busy": "2024-09-12T00:22:31.461608Z",
     "iopub.status.idle": "2024-09-12T00:22:31.466361Z",
     "shell.execute_reply": "2024-09-12T00:22:31.465264Z"
    },
    "papermill": {
     "duration": 0.036743,
     "end_time": "2024-09-12T00:22:31.468510",
     "exception": false,
     "start_time": "2024-09-12T00:22:31.431767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Model predictions and confusion matrix\n",
    "# model.load_state_dict(torch.load(path_checkpoint))\n",
    "# model.eval()\n",
    "# all_preds, all_labels = [], []\n",
    "# with torch.no_grad():\n",
    "#     for batch_X, batch_y in test_loader:\n",
    "#         i_input, q_input = batch_X[:, :, :, 0].unsqueeze(1), batch_X[:, :, :, 1].unsqueeze(1)\n",
    "#         outputs = model(i_input, q_input)\n",
    "#         all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "#         all_labels.extend(batch_y.argmax(1).cpu().numpy())\n",
    "\n",
    "# cm = confusion_matrix(all_labels, all_preds)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=selected_modulation_classes)\n",
    "# disp.plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b77e935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T00:22:31.529505Z",
     "iopub.status.busy": "2024-09-12T00:22:31.529096Z",
     "iopub.status.idle": "2024-09-12T00:22:34.950691Z",
     "shell.execute_reply": "2024-09-12T00:22:34.949667Z"
    },
    "papermill": {
     "duration": 3.45462,
     "end_time": "2024-09-12T00:22:34.952984",
     "exception": false,
     "start_time": "2024-09-12T00:22:31.498364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_23/2769534498.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/kaggle/working/CNN_LSTMmodel.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and load model\n",
    "torch.save(model.state_dict(), '/kaggle/working/CNN_LSTMmodel.pth')\n",
    "loaded_model, _, _, _ = create_model(num_classes)\n",
    "loaded_model.load_state_dict(torch.load('/kaggle/working/CNN_LSTMmodel.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f02640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T00:22:35.016001Z",
     "iopub.status.busy": "2024-09-12T00:22:35.015051Z",
     "iopub.status.idle": "2024-09-12T00:22:35.020192Z",
     "shell.execute_reply": "2024-09-12T00:22:35.019239Z"
    },
    "papermill": {
     "duration": 0.038949,
     "end_time": "2024-09-12T00:22:35.022323",
     "exception": false,
     "start_time": "2024-09-12T00:22:34.983374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Evaluate loaded model\n",
    "# loaded_model.eval()\n",
    "# loaded_model.to(device)\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for batch_X, batch_y in test_loader:\n",
    "#         i_input, q_input = batch_X[:, :, :, 0].unsqueeze(1), batch_X[:, :, :, 1].unsqueeze(1)\n",
    "#         outputs = loaded_model(i_input, q_input)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += batch_y.size(0)\n",
    "#         correct += (predicted == batch_y.argmax(1)).sum().item()\n",
    "\n",
    "# print('Restored model, accuracy: {:.2f}%'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1493018,
     "sourceId": 2468162,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31520.824645,
   "end_time": "2024-09-12T00:22:38.120967",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-11T15:37:17.296322",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
