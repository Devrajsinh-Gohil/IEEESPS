{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8da5ee8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:07.036146Z",
     "iopub.status.busy": "2024-09-12T18:05:07.035679Z",
     "iopub.status.idle": "2024-09-12T18:05:14.644592Z",
     "shell.execute_reply": "2024-09-12T18:05:14.643244Z"
    },
    "papermill": {
     "duration": 7.62064,
     "end_time": "2024-09-12T18:05:14.647994",
     "exception": false,
     "start_time": "2024-09-12T18:05:07.027354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import wandb\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb61e8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:14.664415Z",
     "iopub.status.busy": "2024-09-12T18:05:14.663936Z",
     "iopub.status.idle": "2024-09-12T18:05:14.693021Z",
     "shell.execute_reply": "2024-09-12T18:05:14.691849Z"
    },
    "papermill": {
     "duration": 0.040946,
     "end_time": "2024-09-12T18:05:14.695600",
     "exception": false,
     "start_time": "2024-09-12T18:05:14.654654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import torch.nn as nn\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\" Squeeze-and-Excitation Block \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = self.se(x)\n",
    "        return x * scale\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention Module \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class RadioNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(RadioNet, self).__init__()\n",
    "\n",
    "        # Separate Convolutional Pathways for I and Q\n",
    "        self.q_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(64),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(256),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.i_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(64),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            SEBlock(256),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.feature_size = self._get_conv_output((1, 32, 32))\n",
    "\n",
    "        # Bidirectional LSTM with Layer Normalization\n",
    "        self.lstm = nn.LSTM(self.feature_size * 2, 512, num_layers=2, \n",
    "                            batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.layer_norm = nn.LayerNorm(1024)  # Layer normalization after LSTM\n",
    "\n",
    "        # Multi-Head Attention with multiple heads\n",
    "        self.multi_head_attn = MultiHeadAttention(1024, num_heads=8)\n",
    "\n",
    "        # Enhanced Fully Connected Layers with Dense Connections\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        input = torch.rand(1, *shape)\n",
    "        output = self.q_conv(input)\n",
    "        return int(torch.numel(output) / output.shape[0])\n",
    "\n",
    "    def forward(self, i_input, q_input):\n",
    "        q = self.q_conv(q_input)\n",
    "        q = q.view(q.size(0), -1)\n",
    "\n",
    "        i = self.i_conv(i_input)\n",
    "        i = i.view(i.size(0), -1)\n",
    "\n",
    "        combined = torch.cat((q, i), dim=1)\n",
    "        combined = combined.unsqueeze(1)  # Add sequence dimension\n",
    "\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        # Apply Multi-Head Attention\n",
    "        attn_output = self.multi_head_attn(lstm_out)\n",
    "        context = torch.sum(attn_output, dim=1)  # Sum up the attended output\n",
    "\n",
    "        x = self.fc(context)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# def create_model(num_classes):\n",
    "#     model = RadioNet(num_classes)\n",
    "#     learning_rate = 0.0003\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "#     loss_fn = nn.CrossEntropyLoss()\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "#     return model, optimizer, loss_fn, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc585cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:14.710606Z",
     "iopub.status.busy": "2024-09-12T18:05:14.709168Z",
     "iopub.status.idle": "2024-09-12T18:05:14.939890Z",
     "shell.execute_reply": "2024-09-12T18:05:14.938664Z"
    },
    "papermill": {
     "duration": 0.241064,
     "end_time": "2024-09-12T18:05:14.942902",
     "exception": false,
     "start_time": "2024-09-12T18:05:14.701838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "# Read the dataset\n",
    "dataset_file = h5py.File(\"/kaggle/input/radioml2018/GOLD_XYZ_OSC.0001_1024.hdf5\", \"r\")\n",
    "\n",
    "# Base modulation classes\n",
    "base_modulation_classes = [\n",
    "    'OOK', '4ASK', '8ASK', 'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK',\n",
    "    '16APSK', '32APSK', '64APSK', '128APSK', '16QAM', '32QAM', '64QAM',\n",
    "    '128QAM', '256QAM', 'AM-SSB-WC', 'AM-SSB-SC', 'AM-DSB-WC', 'AM-DSB-SC',\n",
    "    'FM', 'GMSK', 'OQPSK'\n",
    "]\n",
    "\n",
    "# Selected modulation classes\n",
    "selected_modulation_classes = [\n",
    "    '4ASK', 'BPSK', 'QPSK', '16PSK', '16QAM', 'FM', 'AM-DSB-WC', '32APSK'\n",
    "]\n",
    "\n",
    "# Get the indices of selected modulation classes\n",
    "selected_classes_id = [base_modulation_classes.index(cls) for cls in selected_modulation_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c68483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:14.957252Z",
     "iopub.status.busy": "2024-09-12T18:05:14.956527Z",
     "iopub.status.idle": "2024-09-12T18:05:22.456483Z",
     "shell.execute_reply": "2024-09-12T18:05:22.455198Z"
    },
    "papermill": {
     "duration": 7.510778,
     "end_time": "2024-09-12T18:05:22.459485",
     "exception": false,
     "start_time": "2024-09-12T18:05:14.948707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/3816812659.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trained_model.load_state_dict(torch.load('/kaggle/input/radionet/pytorch/default/1/model_checkpoint.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RadioNet(\n",
       "  (q_conv): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1)\n",
       "    (3): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.1)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.1)\n",
       "    (11): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (i_conv): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1)\n",
       "    (3): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.1)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.1)\n",
       "    (11): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(32768, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (multi_head_attn): MultiHeadAttention(\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.1)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.1)\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (10): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (output): Linear(in_features=64, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Classes\n",
    "num_classes = len(selected_modulation_classes)\n",
    "\n",
    "# Load the model\n",
    "trained_model = RadioNet(num_classes=num_classes)\n",
    "\n",
    "# Load the state dict, mapping it to the available device\n",
    "trained_model.load_state_dict(torch.load('/kaggle/input/radionet/pytorch/default/1/model_checkpoint.pth', map_location=device))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0177d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:22.473943Z",
     "iopub.status.busy": "2024-09-12T18:05:22.473454Z",
     "iopub.status.idle": "2024-09-12T18:05:22.479567Z",
     "shell.execute_reply": "2024-09-12T18:05:22.478302Z"
    },
    "papermill": {
     "duration": 0.016277,
     "end_time": "2024-09-12T18:05:22.482095",
     "exception": false,
     "start_time": "2024-09-12T18:05:22.465818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Freeze the parameters of the pre-trained model\n",
    "for param in trained_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3b5db2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:22.497377Z",
     "iopub.status.busy": "2024-09-12T18:05:22.496894Z",
     "iopub.status.idle": "2024-09-12T18:05:22.510138Z",
     "shell.execute_reply": "2024-09-12T18:05:22.508884Z"
    },
    "papermill": {
     "duration": 0.024035,
     "end_time": "2024-09-12T18:05:22.512963",
     "exception": false,
     "start_time": "2024-09-12T18:05:22.488928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FrozenFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            features_list = []\n",
    "            for batch in DataLoader(X, batch_size=32):\n",
    "                batch = batch.to(device)\n",
    "                i_input = batch[:, :, :, 0].unsqueeze(1)\n",
    "                q_input = batch[:, :, :, 1].unsqueeze(1)\n",
    "                \n",
    "                features = self.model.fc(self.model.multi_head_attn(self.model.layer_norm(self.model.lstm(torch.cat((\n",
    "                    self.model.q_conv(q_input).view(q_input.size(0), -1),\n",
    "                    self.model.i_conv(i_input).view(i_input.size(0), -1)\n",
    "                ), dim=1).unsqueeze(1))[0])).sum(dim=1))\n",
    "                \n",
    "                features_list.append(features.cpu().numpy())\n",
    "            \n",
    "            return np.vstack(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75ed2ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:22.527884Z",
     "iopub.status.busy": "2024-09-12T18:05:22.527439Z",
     "iopub.status.idle": "2024-09-12T18:05:22.533957Z",
     "shell.execute_reply": "2024-09-12T18:05:22.532632Z"
    },
    "papermill": {
     "duration": 0.016861,
     "end_time": "2024-09-12T18:05:22.536498",
     "exception": false,
     "start_time": "2024-09-12T18:05:22.519637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "def create_pipeline():\n",
    "    feature_extractor = FrozenFeatureExtractor(trained_model)\n",
    "    xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('feature_extraction', feature_extractor),\n",
    "        ('xgb_classifier', xgb_classifier)\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b373213d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:22.550796Z",
     "iopub.status.busy": "2024-09-12T18:05:22.550366Z",
     "iopub.status.idle": "2024-09-12T18:05:22.564731Z",
     "shell.execute_reply": "2024-09-12T18:05:22.563319Z"
    },
    "papermill": {
     "duration": 0.024924,
     "end_time": "2024-09-12T18:05:22.567552",
     "exception": false,
     "start_time": "2024-09-12T18:05:22.542628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(X, y, n_splits=5, save_path='trained_pipeline.joblib'):\n",
    "    wandb.init(project=\"radioml-xgboost-pipeline\", name=\"xgboost-frozen-feature-pipeline\")\n",
    "    \n",
    "    pipeline = create_pipeline()\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=n_splits, scoring='accuracy')\n",
    "    \n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Mean CV score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"mean_cv_score\": cv_scores.mean(),\n",
    "        \"std_cv_score\": cv_scores.std(),\n",
    "        \"cv_scores\": cv_scores.tolist()\n",
    "    })\n",
    "    \n",
    "    # Train on full dataset\n",
    "    pipeline.fit(X, y)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print(f\"Full dataset accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"full_dataset_accuracy\": accuracy\n",
    "    })\n",
    "    \n",
    "    # Log classification report\n",
    "    class_report = classification_report(y, y_pred, output_dict=True)\n",
    "    wandb.log({\"classification_report\": wandb.Table(dataframe=pd.DataFrame(class_report).transpose())})\n",
    "    \n",
    "    # Log feature importances\n",
    "    feature_imp = pipeline.named_steps['xgb_classifier'].feature_importances_\n",
    "    wandb.log({\"feature_importance\": wandb.plot.bar(\n",
    "        wandb.Table(data=[[f\"feature_{i}\", imp] for i, imp in enumerate(feature_imp)],\n",
    "                    columns=[\"feature\", \"importance\"]),\n",
    "        \"feature\",\n",
    "        \"importance\",\n",
    "        title=\"Feature Importances\"\n",
    "    )})\n",
    "    \n",
    "    # Save the trained pipeline\n",
    "    joblib.dump(pipeline, save_path)\n",
    "    print(f\"Trained pipeline saved to {save_path}\")\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aee5f880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:22.583617Z",
     "iopub.status.busy": "2024-09-12T18:05:22.583139Z",
     "iopub.status.idle": "2024-09-12T18:05:22.591165Z",
     "shell.execute_reply": "2024-09-12T18:05:22.590046Z"
    },
    "papermill": {
     "duration": 0.019259,
     "end_time": "2024-09-12T18:05:22.593759",
     "exception": false,
     "start_time": "2024-09-12T18:05:22.574500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pipeline(load_path='trained_pipeline.joblib'):\n",
    "    if os.path.exists(load_path):\n",
    "        pipeline = joblib.load(load_path)\n",
    "        print(f\"Loaded pipeline from {load_path}\")\n",
    "        return pipeline\n",
    "    else:\n",
    "        print(f\"No saved pipeline found at {load_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "313bf289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:22.609155Z",
     "iopub.status.busy": "2024-09-12T18:05:22.608023Z",
     "iopub.status.idle": "2024-09-12T18:05:24.458812Z",
     "shell.execute_reply": "2024-09-12T18:05:24.457537Z"
    },
    "papermill": {
     "duration": 1.861417,
     "end_time": "2024-09-12T18:05:24.461452",
     "exception": false,
     "start_time": "2024-09-12T18:05:22.600035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "my_secret = user_secrets.get_secret(\"wandb_api_key\") \n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "659e7d78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:24.476623Z",
     "iopub.status.busy": "2024-09-12T18:05:24.476170Z",
     "iopub.status.idle": "2024-09-12T18:05:24.481565Z",
     "shell.execute_reply": "2024-09-12T18:05:24.480380Z"
    },
    "papermill": {
     "duration": 0.015949,
     "end_time": "2024-09-12T18:05:24.484071",
     "exception": false,
     "start_time": "2024-09-12T18:05:24.468122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# class RadioMLDataset(Dataset):\n",
    "#     def __init__(self, X, y):\n",
    "#         self.X = torch.from_numpy(X).float().to(device)\n",
    "#         self.y = torch.from_numpy(y.values).float().to(device)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08699c16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:24.499702Z",
     "iopub.status.busy": "2024-09-12T18:05:24.499279Z",
     "iopub.status.idle": "2024-09-12T18:05:36.852043Z",
     "shell.execute_reply": "2024-09-12T18:05:36.850722Z"
    },
    "papermill": {
     "duration": 12.36433,
     "end_time": "2024-09-12T18:05:36.855117",
     "exception": false,
     "start_time": "2024-09-12T18:05:24.490787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of SNRs (from 30 SNR to 22 SNR)\n",
    "N_SNR = 4 \n",
    "\n",
    "# Initialize placeholders for data\n",
    "X_data = None\n",
    "y_data = None\n",
    "\n",
    "# Loop through selected modulation classes\n",
    "for id in selected_classes_id:\n",
    "    # Load data slices based on indices\n",
    "    X_slice = dataset_file['X'][(106496*(id+1) - 4096*N_SNR) : 106496*(id+1)]\n",
    "    y_slice = dataset_file['Y'][(106496*(id+1) - 4096*N_SNR) : 106496*(id+1)]\n",
    "    \n",
    "    # Concatenate the slices to build the dataset\n",
    "    if X_data is not None:\n",
    "        X_data = np.concatenate([X_data, X_slice], axis=0)\n",
    "        y_data = np.concatenate([y_data, y_slice], axis=0)\n",
    "    else:\n",
    "        X_data = X_slice\n",
    "        y_data = y_slice\n",
    "\n",
    "# Reshape the X_data to the required shape (e.g., 32x32 with 2 channels)\n",
    "X_data = X_data.reshape(len(X_data), 32, 32, 2)\n",
    "\n",
    "# Convert y_data to a DataFrame for easier manipulation\n",
    "y_data_df = pd.DataFrame(y_data)\n",
    "\n",
    "# Drop columns where the sum is 0 (i.e., no modulation class data in that column)\n",
    "for column in y_data_df.columns:\n",
    "    if sum(y_data_df[column]) == 0:\n",
    "        y_data_df = y_data_df.drop(columns=[column])\n",
    "\n",
    "# Assign the remaining columns to match the selected modulation classes\n",
    "y_data_df.columns = selected_modulation_classes\n",
    "\n",
    "y_indices = np.argmax(y_data_df.values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5b3374a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T18:05:36.872088Z",
     "iopub.status.busy": "2024-09-12T18:05:36.871606Z",
     "iopub.status.idle": "2024-09-13T03:28:13.224412Z",
     "shell.execute_reply": "2024-09-13T03:28:13.219313Z"
    },
    "papermill": {
     "duration": 33756.394668,
     "end_time": "2024-09-13T03:28:13.257109",
     "exception": false,
     "start_time": "2024-09-12T18:05:36.862441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdevcode03\u001b[0m (\u001b[33mdevcode03-gujarat-technological-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240912_180536-zsab8qfm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mxgboost-frozen-feature-pipeline\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/radioml-xgboost-pipeline\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/radioml-xgboost-pipeline/runs/zsab8qfm\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.99057791 0.99042533 0.99023423 0.99027237 0.98893721]\n",
      "Mean CV score: 0.9901 (+/- 0.0012)\n",
      "Full dataset accuracy: 0.9960\n",
      "Trained pipeline saved to trained_pipeline.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: full_dataset_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         mean_cv_score ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          std_cv_score ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: full_dataset_accuracy 0.99599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         mean_cv_score 0.99009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          std_cv_score 0.00059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mxgboost-frozen-feature-pipeline\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/radioml-xgboost-pipeline/runs/zsab8qfm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/devcode03-gujarat-technological-university/radioml-xgboost-pipeline\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240912_180536-zsab8qfm/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pipeline from trained_pipeline.joblib\n",
      "Made predictions using loaded pipeline\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "trained_pipeline = train_and_evaluate(X_data, y_indices)\n",
    "\n",
    "# Example of loading the pipeline\n",
    "loaded_pipeline = load_pipeline()\n",
    "if loaded_pipeline:\n",
    "    # Use the loaded pipeline for predictions\n",
    "    new_predictions = loaded_pipeline.predict(X_data)\n",
    "    print(\"Made predictions using loaded pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced15ce",
   "metadata": {
    "papermill": {
     "duration": 0.009395,
     "end_time": "2024-09-13T03:28:13.275779",
     "exception": false,
     "start_time": "2024-09-13T03:28:13.266384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1493018,
     "sourceId": 2468162,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 118112,
     "modelInstanceId": 93901,
     "sourceId": 112031,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33792.100631,
   "end_time": "2024-09-13T03:28:15.922526",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-12T18:05:03.821895",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
